{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Objectifs:\n",
    "#  Programmer sous R:\n",
    "#    1) Une fonction pour la (TRAIN-TEST) validation répétée\n",
    "#    2) Une fonction pour la validation croisée (K feuilles)\n",
    "#    3) Une fonction pour la validation croisée imbriquée\n",
    "#    4) Une fonction pour le Leave One Out (avec k = n)\n",
    "# \n",
    "# deux critères à valider: moyenne des performances et écart-type des \n",
    "\n",
    "# construction de modèles (ou librairies de notre choix):\n",
    "# library(rpart) = algos de décisions -> CART: classification and regression tree (classif ou regression)\n",
    "# library(MASS) = algo de décisions -> LDA : linear decision analysis: (classif binaire)\n",
    "# library(FNN) : K plus proche voisin (k-PPV)\n",
    "# tree <- rpart(X10 ~ ., data= mesData, method= 'class')  ici ~ = en fonction de et . = toutes les autres variables\n",
    "# tree <- rpart(X10 ~ ., data= mesData, method= 'class', control=rpart.control(minsplit=50), cp= val entre 0 et 1)  ici ~ = en fonction de et . = toutes les autres variables\n",
    "# predict(tree, mesNouvellesDonnées, type='class')\n",
    "# predict(tree, mesNouvellesDonnées, type='prob')\n",
    "# JEUX de données madoc: Breiman: wave5000\n",
    "\n",
    "# attente livrable: les fonctions, mini rapport explication fonction et comparaison des modèles(1,2,3,4) :robustesse sur les données de Breiman\n",
    "\n",
    "# en sortie un vecteur mélanger de 1 à n: en entré un entier\n",
    "# fonction sample déjà défini\n",
    "\n",
    "\n",
    "# Question 1\n",
    "#data = dataframe avec une colonne réel\n",
    "#k = le nombre de répétition\n",
    "#predictColumn = string = nom de la colonne X10\n",
    "library(rpart)\n",
    "validationRepeteeRPART = function (data, k) {\n",
    "  errorRate = c();\n",
    "  for (i in 1:k) {\n",
    "    #on mélange les données\n",
    "    randomData = data[sample(nrow(data)),]\n",
    "    #les 20% premières lignes sont les données de tests\n",
    "    testData = randomData[1: 1000,]\n",
    "    #les 80% autres sont les données pour la construction de modèle\n",
    "    modalData = randomData[1000:nrow(data),]\n",
    "    #on calcule la précision (le taux d'erreurs)\n",
    "    tree = rpart(modalData$X10~.,modalData, method=\"class\")\n",
    "    prediction = predict(tree, testData, type=\"class\")\n",
    "    \n",
    "    #utiliser predict pour trouver le taux d'erreur entre data et predict puis utiliser table\n",
    "    contingencyTable = table(testData$X10, prediction)\n",
    "    \n",
    "    #dans une matrice de confusion, les individus en diagonale sont les biens classés donc pour avoir le taux d'erreur on calcule la somme de tous les autres cases/la somme totale de toutes les cases\n",
    "    sumError = 0\n",
    "    total = 0\n",
    "    for (i in 1:nrow(contingencyTable)) {\n",
    "      for (j in 1:ncol(contingencyTable)) {\n",
    "        if (i != j) {\n",
    "          sumError = sumError + contingencyTable[i,j]\n",
    "        }\n",
    "        total = total + contingencyTable[i,j]\n",
    "      }\n",
    "    }\n",
    "    errorRate = c(errorRate, c(sumError/total))\n",
    "  }\n",
    "  return(mean(errorRate))\n",
    "}\n",
    "\n",
    "library(MASS)\n",
    "\n",
    "validationRepeteeLDA = function (data, k) {\n",
    "  errorRate = c()\n",
    "  for (i in 1:k) {\n",
    "    #on mélange les données\n",
    "    randomData = data[sample(nrow(data)),]\n",
    "    #les 20% premières lignes sont les données de tests\n",
    "    testData = randomData[1: 1000,]\n",
    "    #les 80% autres sont les données pour la construction de modèle\n",
    "    modalData = randomData[1000:nrow(data),]\n",
    "    #on calcule la précision (le taux d'erreurs)\n",
    "    tree = lda(modalData$X10~.,modalData, method=\"moment\")\n",
    "    prediction = predict(tree, testData, type=\"moment\")\n",
    "    #utiliser predict pour trouver le taux d'erreur entre data et predict puis utiliser table\n",
    "    contingencyTable = table(testData$X10, prediction$class)\n",
    "    #dans une matrice de confusion, les individus en diagonale sont les biens classés donc pour avoir le taux d'erreur on calcule la somme de tous les autres cases/la somme totale de toutes les cases\n",
    "    sumError = 0\n",
    "    total = 0\n",
    "    for (i in 1:nrow(contingencyTable)) {\n",
    "      for (j in 1:ncol(contingencyTable)) {\n",
    "        if (i != j) {\n",
    "          sumError = sumError + contingencyTable[i,j]\n",
    "        }\n",
    "        total = total + contingencyTable[i,j]\n",
    "      }\n",
    "    }\n",
    "    errorRate = c(errorRate, c(sumError/total))\n",
    "  }\n",
    "  return(mean(errorRate))\n",
    "}\n",
    "\n",
    "#Exercice 2\n",
    "#data = dataframe\n",
    "#partitions = integer\n",
    "validationCroiseeRPART = function(data, partitions, metric) {\n",
    "  errorRate = c()\n",
    "  #on mélange les données\n",
    "  randomData = data[sample(nrow(data)),]\n",
    "  #list\n",
    "  setPartitions = split(randomData, 1:partitions)\n",
    "  for (i in 1:(partitions)) {\n",
    "    tempSetPartitions = setPartitions\n",
    "    currentTestData = setPartitions[[i]]\n",
    "    currentModalDataList = tempSetPartitions[-i]\n",
    "    #fusion dataframe\n",
    "    currentModalData = rbind.data.frame(currentModalDataList[[1]], currentModalDataList[[2]])\n",
    "    \n",
    "    for (j in 3:(partitions-1)) {\n",
    "      currentModalData = rbind.data.frame(currentModalData, currentModalDataList[[j]])\n",
    "    }\n",
    "    \n",
    "    numOfLinesTestCurrentModalData = as.integer(length(currentModalData)*0.2)\n",
    "    numOflinesModalCurrentModalData = as.integer(length(currentModalData)*0.8)\n",
    "    \n",
    "    testData_step2 = currentModalData[1:numOfLinesTestCurrentModalData,]\n",
    "    modalData_step2 = currentModalData[numOfLinesTestCurrentModalData+1:numOflinesModalCurrentModalData,]\n",
    "    \n",
    "    #on calcule la précision (le taux d'erreurs)\n",
    "    tree = rpart(modalData_step2$X10~.,modalData_step2, method=\"class\")\n",
    "    prediction = predict(tree, testData_step2, type=\"class\")\n",
    "    #utiliser predict pour trouver le taux d'erreur entre data et predict puis utiliser table\n",
    "    contingencyTable = table(testData_step2$X10, prediction)\n",
    "    \n",
    "    total = 0\n",
    "    sumError = 0\n",
    "    for (i in 1:nrow(contingencyTable)) {\n",
    "      for (j in 1:ncol(contingencyTable)) {\n",
    "        if (i != j) {\n",
    "          sumError = sumError + contingencyTable[i,j]\n",
    "        }\n",
    "        total = total + contingencyTable[i,j]\n",
    "      }\n",
    "    }\n",
    "    errorRate = c(errorRate, c(sumError/total))\n",
    "    \n",
    "  }\n",
    "  \n",
    "  meanPrecision = mean(errorRate)\n",
    "  robustesse_calculate = 0\n",
    "  \n",
    "  for (i in 1:length(errorRate)) {\n",
    "    robustesse_calculate = robustesse_calculate + (errorRate[i] - meanPrecision)*(errorRate[i] - meanPrecision)\n",
    "  }\n",
    "  #nombre de valeurs dans error rate est la même que le nombre de partitions\n",
    "  robustesse = robustesse_calculate/length(errorRate)\n",
    "  \n",
    "  if (metric == 1) {\n",
    "    return(robustesse)\n",
    "  } else {\n",
    "    return(mean(errorRate))\n",
    "  }\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "validationCroiseeLDA = function(data, partitions, metric) {\n",
    "  errorRate = c()\n",
    "  #on mélange les données\n",
    "  randomData = data[sample(nrow(data)),]\n",
    "  #list\n",
    "  setPartitions = split(randomData, 1:partitions)\n",
    "  for (i in 1:(partitions)) {\n",
    "    tempSetPartitions = setPartitions\n",
    "    currentTestData = setPartitions[[i]]\n",
    "    currentModalDataList = tempSetPartitions[-i]\n",
    "    #fusion dataframe\n",
    "    currentModalData = rbind.data.frame(currentModalDataList[[1]], currentModalDataList[[2]])\n",
    "    \n",
    "    for (j in 3:(partitions-1)) {\n",
    "      currentModalData = rbind.data.frame(currentModalData, currentModalDataList[[j]])\n",
    "    }\n",
    "    \n",
    "    numOfLinesTestCurrentModalData = as.integer(length(currentModalData)*0.2)\n",
    "    numOflinesModalCurrentModalData = as.integer(length(currentModalData)*0.8)\n",
    "    \n",
    "    testData_step2 = currentModalData[1:numOfLinesTestCurrentModalData,]\n",
    "    modalData_step2 = currentModalData[numOfLinesTestCurrentModalData+1:numOflinesModalCurrentModalData,]\n",
    "    \n",
    "    #on calcule la précision (le taux d'erreurs)\n",
    "    tree = lda(X10~.,modalData_step2)\n",
    "    prediction = predict(tree, testData_step2, type=\"class\")\n",
    "    #utiliser predict pour trouver le taux d'erreur entre data et predict puis utiliser table\n",
    "    contingencyTable = table(testData_step2$X10, prediction$class)\n",
    "    \n",
    "    total = 0\n",
    "    sumError = 0\n",
    "    for (i in 1:nrow(contingencyTable)) {\n",
    "      for (j in 1:ncol(contingencyTable)) {\n",
    "        if (i != j) {\n",
    "          sumError = sumError + contingencyTable[i,j]\n",
    "        }\n",
    "        total = total + contingencyTable[i,j]\n",
    "      }\n",
    "    }\n",
    "    errorRate = c(errorRate, c(sumError/total))\n",
    "    \n",
    "  }\n",
    "  \n",
    "  meanPrecision = mean(errorRate)\n",
    "  robustesse_calculate = 0\n",
    "  \n",
    "  for (i in 1:length(errorRate)) {\n",
    "    robustesse_calculate = robustesse_calculate + (errorRate[i] - meanPrecision)*(errorRate[i] - meanPrecision)\n",
    "  }\n",
    "  #nombre de valeurs dans error rate est la même que le nombre de partitions\n",
    "  robustesse = robustesse_calculate/length(errorRate)\n",
    "  \n",
    "  if (metric == 1) {\n",
    "    return(robustesse)\n",
    "  } else {\n",
    "    return(mean(errorRate))\n",
    "  }\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "#appel de fonction validationCroisee défini précédemment\n",
    "#en paramètre k = le nombre de répétition\n",
    "validationCroiseeRepetee = function (data, partitions, k) {\n",
    "  robustesseMean = 0\n",
    "  errorRateMean = 0\n",
    "  \n",
    "  for (i in 1:k) {\n",
    "    robustesseMean = robustesseMean + validationCroiseeRPART(data, partitions, 1)\n",
    "    errorRateMean = errorRateMean + validationCroiseeRPART(data, partitions, 0)\n",
    "  }\n",
    "  \n",
    "  robustesseMean = robustesseMean/k\n",
    "  errorRateMean = errorRateMean/k\n",
    "  \n",
    "  return(c(robustesseMean, errorRateMean))\n",
    "  \n",
    "}\n",
    "\n",
    "#data = dataframe\n",
    "#partitions = nombre de partition où on va split le dataset\n",
    "validationCroiseeImpliquee = function(data, partitions) {\n",
    "  \n",
    "  errorRate = c()\n",
    "  #on mélange les données\n",
    "  randomData = data[sample(nrow(data)),]\n",
    "  #list\n",
    "  setPartitions = split(randomData, 1:partitions)\n",
    "  listErrorRate = c()\n",
    "  \n",
    "  for (i in 1:partitions) {\n",
    "    \n",
    "    errorRate = c()\n",
    "    \n",
    "    for (i in 1:partitions-1) {\n",
    " \n",
    "      tempSetPartitions = setPartitions\n",
    "      currentTestData = setPartitions[[i]]\n",
    "      currentModalDataList = tempSetPartitions[-i]\n",
    "      #fusion dataframe\n",
    "      currentModalData = rbind.data.frame(currentModalDataList[[1]], currentModalDataList[[2]])\n",
    "      \n",
    "      for (j in 3:(partitions-2)) {\n",
    "        currentModalData = rbind.data.frame(currentModalData, currentModalDataList[[j]])\n",
    "      }\n",
    "      \n",
    "      numOfLinesTestCurrentModalData = as.integer(length(currentModalData)*0.2)\n",
    "      numOflinesModalCurrentModalData = as.integer(length(currentModalData)*0.8)\n",
    "      \n",
    "      testData_step2 = currentModalData[1:numOfLinesTestCurrentModalData,]\n",
    "      modalData_step2 = currentModalData[numOfLinesTestCurrentModalData+1:numOflinesModalCurrentModalData,]\n",
    "      \n",
    "      #on calcule la précision (le taux d'erreurs)\n",
    "      tree = lda(X10~.,modalData_step2)\n",
    "      prediction = predict(tree, testData_step2, type=\"class\")\n",
    "      #utiliser predict pour trouver le taux d'erreur entre data et predict puis utiliser table\n",
    "      contingencyTable = table(testData_step2$X10, prediction$class)\n",
    "      \n",
    "      total = 0\n",
    "      sumError = 0\n",
    "      for (i in 1:nrow(contingencyTable)) {\n",
    "        for (j in 1:ncol(contingencyTable)) {\n",
    "          if (i != j) {\n",
    "            sumError = sumError + contingencyTable[i,j]\n",
    "          }\n",
    "          total = total + contingencyTable[i,j]\n",
    "        }\n",
    "      }\n",
    "      errorRate = c(errorRate, c(sumError/total))\n",
    "    }\n",
    "    \n",
    "    listErrorRate = c(c(listErrorRate), mean(errorRate))\n",
    "    \n",
    "  }\n",
    "  \n",
    "  return(listErrorRate)\n",
    "  \n",
    "}\n",
    "\n",
    "#appel de la fonction validationCroisee\n",
    "#en paramètre data: un dataframe\n",
    "leaveOneOut = function(data) {\n",
    "  numberOfrows = nrow(data)\n",
    "  return(c(validationCroiseeRPART(data, numberOfrows, 0), validationCroiseeRPART(data, numberOfrows, 1)))\n",
    "}\n",
    "\n",
    "\n",
    "#appel de tous les fonctions définies précédemment\n",
    "\n",
    "algorithmSelection = function(data, K) {\n",
    "  \n",
    "  validationCroiseeRPARTErrorRate = validationRepeteeLDA(data, K)\n",
    "  validationCroiseeLDAErrorRate = validationRepeteeRPART(data, K)\n",
    "  \n",
    "  if (validationCroiseeRPARTErrorRate > validationCroiseeLDAErrorRate) {\n",
    "    return(\"SOLUTION: LDA\")\n",
    "  } else {\n",
    "    return(\"SOLUTION: RPART\")\n",
    "  }\n",
    "  \n",
    "  return(\"ERROR\")\n",
    "}\n",
    "\n",
    "validationCroiseeGetModel = function(data, partitions, colonneCible, errorRateThreshold = 0.3) {\n",
    "  errorRate = c()\n",
    "  #on m\\u{fffd}lange les donn\\u{fffd}es\n",
    "  randomData = data[sample(nrow(data)),]\n",
    "  #list\n",
    "  setPartitions = split(randomData, 1:partitions)\n",
    "  for (i in 1:(partitions)) {\n",
    "    tempSetPartitions = setPartitions\n",
    "    currentTestData = setPartitions[[i]]\n",
    "    currentModalDataList = tempSetPartitions[-i]\n",
    "    #fusion dataframe\n",
    "    currentModalData = rbind.data.frame(currentModalDataList[[1]], currentModalDataList[[2]])\n",
    "    \n",
    "    for (j in 3:(partitions-1)) {\n",
    "      currentModalData = rbind.data.frame(currentModalData, currentModalDataList[[j]])\n",
    "    }\n",
    "    \n",
    "    numOfLinesTestCurrentModalData = as.integer(length(currentModalData)*0.2)\n",
    "    numOflinesModalCurrentModalData = as.integer(length(currentModalData)*0.8)\n",
    "    \n",
    "    testData_step2 = currentModalData[1:numOfLinesTestCurrentModalData,]\n",
    "    modalData_step2 = currentModalData[numOfLinesTestCurrentModalData+1:numOflinesModalCurrentModalData,]\n",
    "    \n",
    "    #on calcule la pr\\u{fffd}cision (le taux d'erreurs)\n",
    "    #formulaString = paste(\"modalData_step2$\", colonneCible, \"~.\",sep=\"\")\n",
    "    #formulaCible= as.formula(formulaString)\n",
    "    #tree = rpart(formulaCible, modalData_step2, method=\"class\", control=rpart.control(minsplit=5,cp=0))\n",
    "      \n",
    "    tree = rpart(modalData_step2$X1~., modalData_step2, method=\"class\", control=rpart.control(minsplit=5,cp=0))\n",
    "    prediction = predict(tree, testData_step2, type=\"class\")\n",
    "      \n",
    "    #utiliser predict pour trouver le taux d'erreur entre data et predict puis utiliser table\n",
    "    #formulaString = paste(\"testData_step2\", colonneCible,sep=\"$\")\n",
    "    #formulaCible2 = as.formula(formulaString)\n",
    "      \n",
    "    contingencyTable = table(testData_step2$X1, prediction)\n",
    "    \n",
    "    total = 0\n",
    "    sumError = 0\n",
    "    for (i in 1:nrow(contingencyTable)) {\n",
    "      for (j in 1:ncol(contingencyTable)) {\n",
    "        if (i != j) {\n",
    "          sumError = sumError + contingencyTable[i,j]\n",
    "        }\n",
    "        total = total + contingencyTable[i,j]\n",
    "      }\n",
    "    }\n",
    "    errorRate = c(errorRate, c(sumError/total))\n",
    "    \n",
    "  }\n",
    "  \n",
    "  meanPrecision = mean(errorRate)\n",
    "  robustesse_calculate = 0\n",
    "  \n",
    "  for (i in 1:length(errorRate)) {\n",
    "    robustesse_calculate = robustesse_calculate + (errorRate[i] - meanPrecision)*(errorRate[i] - meanPrecision)\n",
    "  }\n",
    "  #nombre de valeurs dans error rate est la m\\u{fffd}me que le nombre de partitions\n",
    "  robustesse = robustesse_calculate/length(errorRate)\n",
    "  \n",
    "  if (mean(errorRate) <= errorRateThreshold) {\n",
    "    listResult = list(mean(errorRate), tree)\n",
    "    print(\"Error Rate < 0.3\")\n",
    "    print(mean(errorRate))\n",
    "    return(listResult)\n",
    "  }else{\n",
    "    listResult = list(0, tree)\n",
    "    print(\"Error Rate > 0.3, WRONG MODEL\")\n",
    "    print(mean(errorRate))\n",
    "    return(listResult)\n",
    "  }\n",
    "}\n",
    "\n",
    "#Variable cible : PREDICTION CHIFFRE D'AFFAIRES\n",
    "GetModelChiffreAffaires = function(data, partitions){\n",
    "  result = 0\n",
    "  while(result == 0){\n",
    "    print(\"generating new model ...\")\n",
    "    donneesAppr = data[ , -c(1,2,3,4,5,6,7,8,9)]\n",
    "    names(donneesAppr) = c(\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\",\"X8\",\"X9\",\"X10\",\"X11\",\"X12\",\"X13\",\"X14\",\"X15\",\"X16\",\"X17\",\"X18\",\"X19\",\"X20\",\"X21\",\"X22\",\"X23\",\"X24\",\"X25\",\"X26\",\"X27\",\"X28\",\"X29\",\"X30\",\"X31\")\n",
    "    listResult = validationCroiseeGetModel(donneesAppr, partitions, \"X1\")\n",
    "    result = listResult[[1]]\n",
    "  }\n",
    "  print(listResult)\n",
    "  donneesTest = test[ , -c(1,2,3,36,37,38,39)]\n",
    "  names(donneesTest) = c(\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\",\"X8\",\"X9\",\"X10\",\"X11\",\"X12\",\"X13\",\"X14\",\"X15\",\"X16\",\"X17\",\"X18\",\"X19\",\"X20\",\"X21\",\"X22\",\"X23\",\"X24\",\"X25\",\"X26\",\"X27\",\"X28\",\"X29\",\"X30\",\"X31\")\n",
    "  predictionChiffreAffaire = predict(listResult[[2]], donneesTest, type=\"class\")\n",
    "  print(predictionChiffreAffaire)\n",
    "}\n",
    "\n",
    "#Variable cible : CAPACITE D'EMPRUNT\n",
    "GetModelCapaciteEmprunt = function(data, partitions){\n",
    "  result = 0\n",
    "  while(result == 0){\n",
    "    print(\"generating new model ...\")\n",
    "    donneesAppr = data[ , -c(1,2,3,4,5,6,7,8,10)]\n",
    "    names(donneesAppr) = c(\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\",\"X8\",\"X9\",\"X10\",\"X11\",\"X12\",\"X13\",\"X14\",\"X15\",\"X16\",\"X17\",\"X18\",\"X19\",\"X20\",\"X21\",\"X22\",\"X23\",\"X24\",\"X25\",\"X26\",\"X27\",\"X28\",\"X29\",\"X30\",\"X31\")\n",
    "    listResult = validationCroiseeGetModel(donneesAppr, partitions, \"X1\", 0.6)\n",
    "    result = listResult[[1]]\n",
    "  }\n",
    "  print(listResult)\n",
    "  donneesTest = test[ , -c(1,2,3,36,37,38,39)]\n",
    "  names(donneesTest) = c(\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\",\"X8\",\"X9\",\"X10\",\"X11\",\"X12\",\"X13\",\"X14\",\"X15\",\"X16\",\"X17\",\"X18\",\"X19\",\"X20\",\"X21\",\"X22\",\"X23\",\"X24\",\"X25\",\"X26\",\"X27\",\"X28\",\"X29\",\"X30\",\"X31\")\n",
    "  predictionCapaciteEmprunt = predict(listResult[[2]], donneesTest, type=\"class\")\n",
    "  print(predictionCapaciteEmprunt)\n",
    "}\n",
    "\n",
    "GetModelCapaciteSecteurOne = function(data, partitions){\n",
    "#Variable cible : SECTEUR 1\n",
    "  result = 0\n",
    "  while(result == 0){\n",
    "    print(\"generating new model ...\")\n",
    "    donneesAppr = data[ , -c(1,3,4,5,6,7,8,9,10)]\n",
    "    names(donneesAppr) = c(\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\",\"X8\",\"X9\",\"X10\",\"X11\",\"X12\",\"X13\",\"X14\",\"X15\",\"X16\",\"X17\",\"X18\",\"X19\",\"X20\",\"X21\",\"X22\",\"X23\",\"X24\",\"X25\",\"X26\",\"X27\",\"X28\",\"X29\",\"X30\",\"X31\")\n",
    "    listResult = validationCroiseeGetModel(donneesAppr, partitions, \"X1\")\n",
    "    result = listResult[[1]]\n",
    "  }\n",
    "  print(listResult)\n",
    "  donneesTest = test[ , -c(1,2,3,36,37,38,39)]\n",
    "  names(donneesTest) = c(\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\",\"X8\",\"X9\",\"X10\",\"X11\",\"X12\",\"X13\",\"X14\",\"X15\",\"X16\",\"X17\",\"X18\",\"X19\",\"X20\",\"X21\",\"X22\",\"X23\",\"X24\",\"X25\",\"X26\",\"X27\",\"X28\",\"X29\",\"X30\",\"X31\")\n",
    "  predictionChiffreAffaire = predict(listResult[[2]], donneesTest, type=\"class\")\n",
    "  print(predictionChiffreAffaire)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Error Rate < 0.3\"\n",
      "[1] 0.2333333\n",
      "[[1]]\n",
      "[1] 0.2333333\n",
      "\n",
      "[[2]]\n",
      "n= 25 \n",
      "\n",
      "node), split, n, loss, yval, (yprob)\n",
      "      * denotes terminal node\n",
      "\n",
      " 1) root 25 15 0 (0.4 0.12 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04)  \n",
      "   2) X2< 0.5 10  0 0 (1 0 0 0 0 0 0 0 0 0 0 0 0 0) *\n",
      "   3) X2>=0.5 15 12 17500 (0 0.2 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067)  \n",
      "     6) X2< 1.5 6  3 17500 (0 0.5 0.17 0.17 0 0 0.17 0 0 0 0 0 0 0)  \n",
      "      12) X6< 445.715 3  0 17500 (0 1 0 0 0 0 0 0 0 0 0 0 0 0) *\n",
      "      13) X6>=445.715 3  2 20000 (0 0 0.33 0.33 0 0 0.33 0 0 0 0 0 0 0) *\n",
      "     7) X2>=1.5 9  8 35000 (0 0 0 0 0.11 0.11 0 0.11 0.11 0.11 0.11 0.11 0.11 0.11)  \n",
      "      14) X2< 2.5 2  1 35000 (0 0 0 0 0.5 0.5 0 0 0 0 0 0 0 0) *\n",
      "      15) X2>=2.5 7  6 52500 (0 0 0 0 0 0 0 0.14 0.14 0.14 0.14 0.14 0.14 0.14)  \n",
      "        30) X2< 4 2  1 52500 (0 0 0 0 0 0 0 0.5 0.5 0 0 0 0 0) *\n",
      "        31) X2>=4 5  4 101000 (0 0 0 0 0 0 0 0 0 0.2 0.2 0.2 0.2 0.2)  \n",
      "          62) X2< 7 2  1 101000 (0 0 0 0 0 0 0 0 0 0.5 0.5 0 0 0) *\n",
      "          63) X2>=7 3  2 160000 (0 0 0 0 0 0 0 0 0 0 0 0.33 0.33 0.33) *\n",
      "\n",
      "     1      2      3      4      5      6      7      8      9     10     11 \n",
      "     0  20000  52500  17500      0  35000  52500      0  17500  20000 101000 \n",
      "    12     13     14     15     16     17     18     19     20     21     22 \n",
      " 17500 101000  35000  17500      0  52500  52500 160000  20000 160000  17500 \n",
      "    23     24     25     26     27     28     29     30     31     32     33 \n",
      " 17500      0      0      0 160000      0  35000  35000 101000  52500 101000 \n",
      "    34     35     36     37     38     39     40     41     42     43     44 \n",
      " 17500  17500  20000  17500  17500 160000 160000  20000 160000 160000 160000 \n",
      "    45     46     47     48     49     50     51     52     53     54     55 \n",
      "101000  35000 160000  17500 160000 160000 160000  17500  17500  52500 160000 \n",
      "    56     57     58     59     60     61     62     63     64     65     66 \n",
      " 17500 160000      0 101000 160000  17500      0      0 160000 160000      0 \n",
      "    67     68     69     70     71     72     73     74     75     76     77 \n",
      " 17500 160000  35000  35000  17500 160000      0 160000      0      0      0 \n",
      "    78     79     80     81     82     83     84     85     86     87     88 \n",
      "     0      0      0      0      0      0      0      0      0      0  17500 \n",
      "    89     90     91     92     93     94     95     96     97     98     99 \n",
      "     0 160000 101000 160000      0 101000 101000 160000 101000  17500  52500 \n",
      "   100    101    102    103    104    105    106    107    108    109    110 \n",
      "     0 101000 160000 160000 160000  17500      0      0  35000  52500  35000 \n",
      "   111    112    113    114    115    116    117    118    119    120    121 \n",
      " 17500  17500      0  17500      0 101000  52500  17500      0      0  35000 \n",
      "   122    123    124    125    126    127    128    129    130    131    132 \n",
      "101000      0  52500      0      0  17500  35000 160000 160000 101000  20000 \n",
      "   133    134    135    136    137    138    139    140    141    142    143 \n",
      " 17500  52500  20000 160000  17500  17500 160000 160000 160000  35000      0 \n",
      "   144    145    146    147    148    149    150    151    152    153    154 \n",
      "     0 160000 160000 160000  17500  35000  17500 160000 160000  17500 101000 \n",
      "   155    156    157    158    159    160    161    162    163    164    165 \n",
      "     0  35000 160000 160000 160000  17500  17500      0  20000 101000      0 \n",
      "   166    167    168    169    170    171    172    173    174    175    176 \n",
      "     0      0      0      0 160000 101000      0      0      0      0      0 \n",
      "   177    178    179    180    181    182    183    184    185    186    187 \n",
      " 17500      0      0      0      0 160000      0  35000      0 101000  17500 \n",
      "   188    189    190    191    192    193    194    195    196    197    198 \n",
      "101000 160000  17500 101000 160000  35000 101000 101000      0 101000      0 \n",
      "   199    200    201    202    203    204    205    206    207    208    209 \n",
      "160000  17500  35000      0      0  35000  17500  52500  20000  17500      0 \n",
      "   210    211    212    213    214    215    216    217    218    219    220 \n",
      "     0      0      0      0 160000  35000  17500      0  35000      0 160000 \n",
      "   221    222    223    224    225    226    227    228    229    230    231 \n",
      "101000 160000  17500  52500      0  17500  52500  17500      0 101000  17500 \n",
      "   232    233    234    235    236    237    238    239    240    241    242 \n",
      " 35000  20000  35000  17500  20000  20000      0      0      0  17500  17500 \n",
      "   243    244    245    246    247    248    249    250    251    252    253 \n",
      "101000      0      0      0      0      0  35000  20000      0      0      0 \n",
      "   254    255    256    257    258    259    260    261    262    263    264 \n",
      "     0 160000 160000 160000      0 160000  35000 101000 101000      0 160000 \n",
      "   265    266    267    268    269    270    271    272    273    274    275 \n",
      " 35000      0      0  17500 160000  52500 160000 160000  52500      0      0 \n",
      "   276    277    278    279    280    281    282    283    284    285    286 \n",
      "     0      0  20000      0      0      0      0  52500  17500 101000  20000 \n",
      "   287    288    289    290    291    292    293    294    295    296    297 \n",
      "     0      0  17500 160000 101000  20000  17500      0  35000      0  35000 \n",
      "   298    299    300    301    302    303    304    305    306    307    308 \n",
      "     0  52500      0 160000  35000  35000 101000      0  35000      0  17500 \n",
      "   309    310    311    312    313    314    315    316    317    318    319 \n",
      "     0  35000      0 101000      0 160000      0      0      0  52500 160000 \n",
      "   320    321    322    323    324    325    326    327    328    329    330 \n",
      "     0      0      0 101000 160000      0  17500 101000  52500 160000 101000 \n",
      "   331    332    333    334    335    336    337    338    339    340    341 \n",
      "     0 101000      0  17500 160000      0      0  17500 160000 160000 160000 \n",
      "   342    343    344    345    346    347    348    349    350    351    352 \n",
      "     0  35000 160000  52500 160000 160000  17500 160000 160000  35000  20000 \n",
      "   353    354    355    356    357    358    359    360    361    362    363 \n",
      "     0  17500  35000  17500 101000      0 101000 160000 101000  17500  17500 \n",
      "   364    365    366    367    368    369    370    371    372    373    374 \n",
      "160000 160000 101000 160000      0  17500  52500      0      0 160000      0 \n",
      "   375    376    377    378    379    380    381    382    383    384    385 \n",
      "101000  17500  20000      0      0  17500  17500  35000 101000  17500 101000 \n",
      "   386    387    388    389    390    391    392    393    394    395    396 \n",
      "160000      0      0      0      0      0      0      0      0      0      0 \n",
      "   397    398    399    400    401    402    403    404    405    406    407 \n",
      "     0      0 160000 101000  35000  17500      0  17500 160000  17500  52500 \n",
      "   408    409    410    411    412    413    414    415    416    417    418 \n",
      " 17500  17500      0  17500 101000  17500  17500      0      0  20000  20000 \n",
      "   419    420    421    422    423    424    425    426    427    428    429 \n",
      " 17500  17500  17500  20000  17500  35000  17500  17500  17500  17500  17500 \n",
      "   430    431    432    433    434    435    436    437    438    439    440 \n",
      " 52500  35000  17500  35000  17500  17500      0  17500 101000  35000 160000 \n",
      "   441    442    443    444    445    446    447    448    449    450    451 \n",
      " 17500  52500      0  17500  35000      0  17500 160000  35000 101000  17500 \n",
      "   452    453    454    455    456    457    458    459    460    461    462 \n",
      " 20000  52500 160000  17500  35000  17500  17500      0 101000  17500 160000 \n",
      "   463    464    465    466    467    468    469    470    471    472    473 \n",
      " 35000 160000  17500 101000  17500  20000  52500  20000 101000  17500  20000 \n",
      "   474    475    476    477    478    479    480    481    482    483    484 \n",
      " 20000  20000  20000  17500  17500      0 160000      0      0      0      0 \n",
      "   485    486    487    488    489    490    491    492    493    494    495 \n",
      "     0      0  52500  17500      0      0      0  20000 101000  17500 160000 \n",
      "   496    497    498    499    500    501    502    503    504    505    506 \n",
      "160000 160000      0      0      0      0 160000      0      0      0      0 \n",
      "   507    508    509    510    511    512    513    514    515    516    517 \n",
      "     0      0  20000      0  20000      0  17500      0      0      0 160000 \n",
      "   518    519    520    521    522    523    524    525    526    527    528 \n",
      "     0 160000  17500      0      0      0      0      0      0 160000      0 \n",
      "   529    530    531    532    533    534    535    536    537    538    539 \n",
      "     0 160000  52500  20000      0  35000 160000  17500  20000  20000      0 \n",
      "   540    541    542    543    544    545    546    547    548    549    550 \n",
      " 17500  35000  35000      0      0      0      0      0      0  20000      0 \n",
      "   551    552    553    554    555    556    557    558    559    560    561 \n",
      " 20000  35000      0  20000  20000      0      0      0  17500  20000  17500 \n",
      "   562    563    564    565    566    567    568    569    570    571    572 \n",
      "     0      0  20000      0  17500      0      0  20000  17500  17500  17500 \n",
      "   573    574    575    576    577    578    579    580    581    582    583 \n",
      " 17500      0 101000      0 101000      0 160000      0      0      0 101000 \n",
      "   584    585    586    587    588    589    590    591    592    593    594 \n",
      "     0      0  17500      0      0  17500      0  17500      0  17500      0 \n",
      "   595    596    597    598    599    600    601    602    603    604    605 \n",
      "     0      0 160000      0  17500      0  20000 160000  17500  17500 160000 \n",
      "   606    607    608    609    610    611    612    613    614    615    616 \n",
      " 17500  20000 160000  20000  52500      0  17500  35000      0  35000      0 \n",
      "   617    618    619    620    621    622    623    624    625    626    627 \n",
      "160000      0 101000      0      0 101000      0  17500 160000      0  52500 \n",
      "   628    629    630    631    632    633    634    635    636    637    638 \n",
      " 17500  17500  17500  52500      0      0  17500      0 160000  35000 160000 \n",
      "   639    640    641    642    643    644    645    646    647    648    649 \n",
      "     0      0      0 160000 160000  17500  17500      0 101000      0  52500 \n",
      "   650    651    652    653    654    655    656    657    658    659    660 \n",
      "     0 160000  52500  52500  20000 160000  17500  35000  17500  17500      0 \n",
      "   661    662    663    664    665    666    667    668    669    670    671 \n",
      " 52500      0  52500 101000  35000  52500  35000  17500  17500 160000  35000 \n",
      "   672    673    674    675    676    677    678    679    680    681    682 \n",
      "160000 160000 160000  17500 101000      0  20000  17500  52500      0 101000 \n",
      "   683    684    685    686    687    688    689    690    691    692    693 \n",
      "     0      0 160000 101000      0 101000  20000  17500      0  52500  35000 \n",
      "   694    695    696    697    698    699    700    701    702    703    704 \n",
      "     0 101000  20000  20000  35000 101000      0      0      0  17500  17500 \n",
      "   705    706    707    708    709    710    711    712    713    714    715 \n",
      "     0  17500  17500  17500  20000      0      0  35000      0 101000      0 \n",
      "   716    717    718    719    720    721    722    723    724    725    726 \n",
      "     0  17500      0  35000      0  35000      0  17500  20000      0  35000 \n",
      "   727    728    729    730    731    732    733    734    735    736    737 \n",
      "     0 101000      0 101000  35000 101000 101000  17500  17500  52500  17500 \n",
      "   738    739    740    741    742    743    744    745    746    747    748 \n",
      " 20000      0 160000      0      0  52500      0      0      0      0 101000 \n",
      "   749    750    751    752    753    754    755    756    757    758    759 \n",
      " 17500      0  17500  52500      0  17500      0  20000  17500  52500  35000 \n",
      "   760    761    762    763    764    765    766    767    768    769    770 \n",
      " 17500  52500      0  17500      0 101000      0  52500 160000      0      0 \n",
      "   771    772    773    774    775    776    777    778    779    780    781 \n",
      "     0 101000      0  17500  17500 160000      0  17500      0  20000      0 \n",
      "   782    783    784    785    786    787    788    789    790    791    792 \n",
      "     0  20000 160000  52500  35000  20000  17500  17500  17500      0 160000 \n",
      "   793    794    795    796    797    798    799    800    801    802    803 \n",
      "101000  35000 160000  17500  17500  35000      0  17500  17500 160000  17500 \n",
      "   804    805    806    807    808    809    810    811    812    813    814 \n",
      "     0  20000      0  52500  17500      0 101000 160000 160000 160000      0 \n",
      "   815    816    817    818    819    820    821    822    823    824    825 \n",
      "     0 101000      0      0  20000  35000  17500 101000  35000  20000  17500 \n",
      "   826    827    828    829    830    831    832    833    834    835    836 \n",
      "101000  35000      0  35000      0      0  52500  35000  17500 101000  17500 \n",
      "   837    838    839    840    841    842    843    844    845    846    847 \n",
      " 17500 101000  17500 160000      0  35000  17500  52500 101000  20000      0 \n",
      "   848    849    850    851    852    853    854    855    856    857    858 \n",
      " 35000  52500 160000      0 101000      0  52500  17500      0  52500      0 \n",
      "   859    860    861    862    863    864    865    866    867    868    869 \n",
      " 17500      0  17500      0  17500      0  17500  17500  17500      0  17500 \n",
      "   870    871    872    873    874    875    876    877    878    879    880 \n",
      "160000  35000  17500      0      0  17500      0      0      0      0      0 \n",
      "   881    882    883    884    885    886    887    888    889    890    891 \n",
      "160000      0      0      0      0      0      0      0      0      0      0 \n",
      "   892    893    894    895    896    897    898    899    900    901    902 \n",
      "     0      0      0      0      0      0  52500      0      0 101000  35000 \n",
      "   903    904    905    906    907    908    909    910    911    912    913 \n",
      "     0      0  17500      0      0  52500      0      0  17500  17500  17500 \n",
      "   914    915    916    917    918    919    920    921    922    923    924 \n",
      " 35000 160000  17500  20000  17500 160000  20000      0  17500  52500      0 \n",
      "   925    926    927    928    929    930    931    932    933    934    935 \n",
      " 20000  17500      0      0  17500      0  17500  17500  17500  52500  17500 \n",
      "   936    937    938    939    940    941    942    943    944    945    946 \n",
      " 17500  17500      0      0      0      0  17500      0  17500 160000      0 \n",
      "   947    948    949    950    951    952    953    954    955    956    957 \n",
      "160000 101000  52500 101000      0 160000      0 101000      0  20000  17500 \n",
      "   958    959    960    961    962    963    964    965    966    967    968 \n",
      "     0  17500 160000 160000  17500      0      0      0      0      0      0 \n",
      "   969    970    971    972    973    974    975    976    977    978    979 \n",
      "101000      0      0      0      0      0  17500  17500  35000  17500  17500 \n",
      "   980    981    982    983    984    985    986    987    988    989    990 \n",
      "160000  20000 101000 160000  17500  20000  52500  52500 160000  52500  52500 \n",
      "   991    992    993    994    995    996    997    998    999   1000 \n",
      " 17500  35000 160000  17500      0      0      0  17500  17500  17500 \n",
      "14 Levels: 0 17500 20000 22400 35000 40000 42400 52500 67200 101000 ... 448000\n"
     ]
    }
   ],
   "source": [
    "apprentissage <- read.table(\"apprentissage.csv\", sep = \",\" ,header = FALSE)\n",
    "test <- read.table(\"test.csv\", sep = \",\" ,header = FALSE)\n",
    "GetModelChiffreAffaires(apprentissage, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Error Rate > 0.3, WRONG MODEL\"\n",
      "[1] 0.85\n",
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Error Rate > 0.3, WRONG MODEL\"\n",
      "[1] 1\n",
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Error Rate > 0.3, WRONG MODEL\"\n",
      "[1] 0.6833333\n",
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Error Rate > 0.3, WRONG MODEL\"\n",
      "[1] 0.9833333\n",
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Error Rate > 0.3, WRONG MODEL\"\n",
      "[1] 0.85\n",
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Error Rate > 0.3, WRONG MODEL\"\n",
      "[1] 1\n",
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Error Rate > 0.3, WRONG MODEL\"\n",
      "[1] 1\n",
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Error Rate > 0.3, WRONG MODEL\"\n",
      "[1] 0.7\n",
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Error Rate > 0.3, WRONG MODEL\"\n",
      "[1] 0.6833333\n",
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Error Rate < 0.3\"\n",
      "[1] 0.5333333\n",
      "[[1]]\n",
      "[1] 0.5333333\n",
      "\n",
      "[[2]]\n",
      "n= 25 \n",
      "\n",
      "node), split, n, loss, yval, (yprob)\n",
      "      * denotes terminal node\n",
      "\n",
      " 1) root 25 14 10000 (0.04 0.24 0.04 0.44 0.04 0.04 0.04 0.04 0.04 0.04)  \n",
      "   2) X28< 56.425 14  4 10000 (0 0.14 0.071 0.71 0 0.071 0 0 0 0)  \n",
      "     4) X23< 814.595 12  2 10000 (0 0.083 0.083 0.83 0 0 0 0 0 0)  \n",
      "       8) X26< 1.58 5  2 10000 (0 0.2 0.2 0.6 0 0 0 0 0 0)  \n",
      "        16) X2< 0.5 2  1 5000 (0 0.5 0.5 0 0 0 0 0 0 0) *\n",
      "        17) X2>=0.5 3  0 10000 (0 0 0 1 0 0 0 0 0 0) *\n",
      "       9) X26>=1.58 7  0 10000 (0 0 0 1 0 0 0 0 0 0) *\n",
      "     5) X23>=814.595 2  1 5000 (0 0.5 0 0 0 0.5 0 0 0 0) *\n",
      "   3) X28>=56.425 11  7 5000 (0.091 0.36 0 0.091 0.091 0 0.091 0.091 0.091 0.091)  \n",
      "     6) X10< 40.3 5  1 5000 (0 0.8 0 0.2 0 0 0 0 0 0) *\n",
      "     7) X10>=40.3 6  5 3000 (0.17 0 0 0 0.17 0 0.17 0.17 0.17 0.17)  \n",
      "      14) X2< 6.5 3  2 3000 (0.33 0 0 0 0.33 0 0 0.33 0 0) *\n",
      "      15) X2>=6.5 3  2 30000 (0 0 0 0 0 0 0.33 0 0.33 0.33) *\n",
      "\n",
      "    1     2     3     4     5     6     7     8     9    10    11    12    13 \n",
      " 5000  3000  3000 10000 10000  5000 10000 10000  5000 10000 10000 10000  3000 \n",
      "   14    15    16    17    18    19    20    21    22    23    24    25    26 \n",
      " 5000 10000  5000 10000 10000 30000  3000  5000 10000  5000  5000  5000  5000 \n",
      "   27    28    29    30    31    32    33    34    35    36    37    38    39 \n",
      "30000  5000 10000  5000 10000  5000 10000 10000 10000  5000 10000  5000 30000 \n",
      "   40    41    42    43    44    45    46    47    48    49    50    51    52 \n",
      " 5000  5000 10000  5000 30000 10000 10000  5000 10000 30000 10000 10000  5000 \n",
      "   53    54    55    56    57    58    59    60    61    62    63    64    65 \n",
      "10000 10000  5000 10000 30000  5000  3000  5000  5000 10000  5000 30000 10000 \n",
      "   66    67    68    69    70    71    72    73    74    75    76    77    78 \n",
      " 5000 10000  5000  5000  5000 10000 30000  3000 30000  5000 10000 10000 10000 \n",
      "   79    80    81    82    83    84    85    86    87    88    89    90    91 \n",
      " 5000  3000 10000  5000  3000  3000  5000 10000  5000 10000 10000  5000  5000 \n",
      "   92    93    94    95    96    97    98    99   100   101   102   103   104 \n",
      "30000  5000  5000 10000  5000  5000 10000  3000  5000  3000 30000 30000 10000 \n",
      "  105   106   107   108   109   110   111   112   113   114   115   116   117 \n",
      "10000  5000 10000  3000  3000  3000 10000  3000  5000  5000  5000  3000 10000 \n",
      "  118   119   120   121   122   123   124   125   126   127   128   129   130 \n",
      "10000  5000 10000 10000  5000  5000 10000  3000 10000  5000  5000 10000 30000 \n",
      "  131   132   133   134   135   136   137   138   139   140   141   142   143 \n",
      " 5000  5000 10000 10000  5000  5000 10000  5000 30000  5000  5000  5000  5000 \n",
      "  144   145   146   147   148   149   150   151   152   153   154   155   156 \n",
      "10000 10000 30000 30000 10000  5000 10000  5000 30000 10000  3000 10000 10000 \n",
      "  157   158   159   160   161   162   163   164   165   166   167   168   169 \n",
      "10000 30000 30000  5000 10000  5000  5000 10000  5000 10000 10000  3000  3000 \n",
      "  170   171   172   173   174   175   176   177   178   179   180   181   182 \n",
      " 5000  3000  5000  5000  5000 10000  5000  5000 10000  3000 10000 10000  5000 \n",
      "  183   184   185   186   187   188   189   190   191   192   193   194   195 \n",
      " 5000 10000 10000 10000 10000  3000 10000 10000  3000 10000 10000  5000 10000 \n",
      "  196   197   198   199   200   201   202   203   204   205   206   207   208 \n",
      "10000 10000 10000 30000 10000 10000  5000  5000  5000  5000 10000 10000 10000 \n",
      "  209   210   211   212   213   214   215   216   217   218   219   220   221 \n",
      "10000  5000  5000  5000  5000  5000 10000 10000  5000 10000 10000  5000 10000 \n",
      "  222   223   224   225   226   227   228   229   230   231   232   233   234 \n",
      "30000 10000  3000  5000 10000 10000 10000  5000  5000  3000  5000 10000  5000 \n",
      "  235   236   237   238   239   240   241   242   243   244   245   246   247 \n",
      "10000 10000  5000 10000  5000 10000 10000 10000 10000  5000 10000 10000  5000 \n",
      "  248   249   250   251   252   253   254   255   256   257   258   259   260 \n",
      " 5000 10000  3000  3000  5000 10000 10000 30000 10000  5000  5000 10000 10000 \n",
      "  261   262   263   264   265   266   267   268   269   270   271   272   273 \n",
      " 5000 10000  5000  5000  5000  5000  5000 10000 10000 10000 10000  5000  5000 \n",
      "  274   275   276   277   278   279   280   281   282   283   284   285   286 \n",
      "10000 10000 10000  5000 10000  5000 10000  5000 10000 10000 10000  5000 10000 \n",
      "  287   288   289   290   291   292   293   294   295   296   297   298   299 \n",
      "10000 10000  3000 10000  3000  3000  3000 10000  3000  5000 10000  5000  3000 \n",
      "  300   301   302   303   304   305   306   307   308   309   310   311   312 \n",
      " 5000  5000 10000  5000  3000 10000  5000  5000 10000  5000  5000 10000  3000 \n",
      "  313   314   315   316   317   318   319   320   321   322   323   324   325 \n",
      " 5000 10000 10000  5000  5000  5000 10000 10000  5000 10000  5000  5000 10000 \n",
      "  326   327   328   329   330   331   332   333   334   335   336   337   338 \n",
      "10000  3000 10000 30000  5000 10000  5000  5000 10000 30000 10000 10000 10000 \n",
      "  339   340   341   342   343   344   345   346   347   348   349   350   351 \n",
      "10000  5000  5000  3000  5000 30000  3000 10000 10000 10000 30000 10000 10000 \n",
      "  352   353   354   355   356   357   358   359   360   361   362   363   364 \n",
      "10000  5000 10000 10000 10000  3000 10000  3000 30000  5000  5000 10000 10000 \n",
      "  365   366   367   368   369   370   371   372   373   374   375   376   377 \n",
      "10000 10000 30000  5000 10000  5000  5000 10000 30000  5000  3000 10000  5000 \n",
      "  378   379   380   381   382   383   384   385   386   387   388   389   390 \n",
      " 5000  5000 10000 10000 10000 10000  5000  5000 30000  5000  5000  5000  5000 \n",
      "  391   392   393   394   395   396   397   398   399   400   401   402   403 \n",
      "10000  5000  5000  5000  5000  5000  5000  5000 10000 10000  3000 10000  3000 \n",
      "  404   405   406   407   408   409   410   411   412   413   414   415   416 \n",
      "10000 10000 10000  5000 10000 10000 10000 10000 10000 10000  5000  5000 10000 \n",
      "  417   418   419   420   421   422   423   424   425   426   427   428   429 \n",
      " 5000  3000 10000 10000 10000  3000 10000  5000 10000 10000 10000 10000 10000 \n",
      "  430   431   432   433   434   435   436   437   438   439   440   441   442 \n",
      "10000  5000 10000  5000  5000 10000  5000  5000  3000  5000 10000 10000 10000 \n",
      "  443   444   445   446   447   448   449   450   451   452   453   454   455 \n",
      "10000  5000  5000  5000 10000  5000  5000  5000 10000  5000  5000 10000  5000 \n",
      "  456   457   458   459   460   461   462   463   464   465   466   467   468 \n",
      "10000 10000 10000  5000  5000 10000 30000 10000  5000  5000  5000 10000 10000 \n",
      "  469   470   471   472   473   474   475   476   477   478   479   480   481 \n",
      " 5000  3000  5000 10000 10000 10000 10000 10000 10000 10000  5000 30000 10000 \n",
      "  482   483   484   485   486   487   488   489   490   491   492   493   494 \n",
      " 5000 10000  5000  5000 10000  5000  3000  5000  5000  5000  5000  5000 10000 \n",
      "  495   496   497   498   499   500   501   502   503   504   505   506   507 \n",
      " 5000  5000  5000  5000  5000  5000  5000 30000  5000 10000 10000  5000  5000 \n",
      "  508   509   510   511   512   513   514   515   516   517   518   519   520 \n",
      " 5000 10000  5000 10000 10000  5000  5000  3000 10000  5000  5000 30000  5000 \n",
      "  521   522   523   524   525   526   527   528   529   530   531   532   533 \n",
      "10000 10000  5000  5000 10000 10000  5000 10000 10000  5000 10000  5000 10000 \n",
      "  534   535   536   537   538   539   540   541   542   543   544   545   546 \n",
      "10000 10000  5000  5000  5000 10000  5000  5000  5000 10000  5000  5000  5000 \n",
      "  547   548   549   550   551   552   553   554   555   556   557   558   559 \n",
      "10000  5000  5000 10000  5000 10000 10000  5000  5000  5000  5000  5000 10000 \n",
      "  560   561   562   563   564   565   566   567   568   569   570   571   572 \n",
      " 5000 10000  5000  5000 10000  5000  5000 10000  5000  3000 10000 10000 10000 \n",
      "  573   574   575   576   577   578   579   580   581   582   583   584   585 \n",
      "10000  5000  3000  5000  5000  5000 10000  5000 10000 10000 10000  5000 10000 \n",
      "  586   587   588   589   590   591   592   593   594   595   596   597   598 \n",
      "10000 10000 10000 10000  5000 10000  5000 10000 10000 10000  5000  5000  5000 \n",
      "  599   600   601   602   603   604   605   606   607   608   609   610   611 \n",
      "10000 10000 10000  5000 10000 10000  5000  5000  3000  5000  5000  3000  5000 \n",
      "  612   613   614   615   616   617   618   619   620   621   622   623   624 \n",
      " 3000 10000  5000  3000  5000 10000  5000 10000  5000  5000  5000  5000 10000 \n",
      "  625   626   627   628   629   630   631   632   633   634   635   636   637 \n",
      " 5000 10000  5000 10000 10000  5000  5000 10000  5000 10000  5000  5000  3000 \n",
      "  638   639   640   641   642   643   644   645   646   647   648   649   650 \n",
      " 5000  5000 10000  5000 30000  5000 10000 10000  5000  5000  5000 10000  5000 \n",
      "  651   652   653   654   655   656   657   658   659   660   661   662   663 \n",
      "10000 10000  3000  5000  5000 10000 10000 10000 10000  5000 10000 10000  3000 \n",
      "  664   665   666   667   668   669   670   671   672   673   674   675   676 \n",
      " 5000 10000  3000  5000  5000 10000  5000  5000  5000 10000  5000 10000  3000 \n",
      "  677   678   679   680   681   682   683   684   685   686   687   688   689 \n",
      "10000 10000 10000 10000  5000  5000 10000  5000 30000  5000 10000 10000 10000 \n",
      "  690   691   692   693   694   695   696   697   698   699   700   701   702 \n",
      "10000  5000 10000  5000 10000 10000 10000 10000  5000  5000  5000  5000  5000 \n",
      "  703   704   705   706   707   708   709   710   711   712   713   714   715 \n",
      "10000 10000  5000  5000 10000 10000 10000  5000  5000  5000 10000 10000  5000 \n",
      "  716   717   718   719   720   721   722   723   724   725   726   727   728 \n",
      " 5000 10000  5000  3000  5000 10000  3000 10000  5000  5000  5000  5000 10000 \n",
      "  729   730   731   732   733   734   735   736   737   738   739   740   741 \n",
      "10000 10000  5000  5000  5000  3000 10000 10000  5000  3000  3000 10000  5000 \n",
      "  742   743   744   745   746   747   748   749   750   751   752   753   754 \n",
      " 5000 10000  5000  5000  3000 10000 10000  5000 10000  5000  3000  5000  5000 \n",
      "  755   756   757   758   759   760   761   762   763   764   765   766   767 \n",
      " 5000  5000 10000 10000 10000 10000  3000  5000  3000  5000  5000 10000  5000 \n",
      "  768   769   770   771   772   773   774   775   776   777   778   779   780 \n",
      " 5000 10000 10000 10000 10000 10000 10000 10000 10000  5000 10000  5000  5000 \n",
      "  781   782   783   784   785   786   787   788   789   790   791   792   793 \n",
      "10000  5000 10000  5000 10000  3000 10000 10000  5000 10000 10000 10000  3000 \n",
      "  794   795   796   797   798   799   800   801   802   803   804   805   806 \n",
      "10000  5000 10000  5000 10000 10000 10000 10000 10000 10000 10000 10000 10000 \n",
      "  807   808   809   810   811   812   813   814   815   816   817   818   819 \n",
      " 5000 10000 10000 10000  5000 10000 10000  5000  5000  3000  5000 10000  3000 \n",
      "  820   821   822   823   824   825   826   827   828   829   830   831   832 \n",
      "10000 10000  3000 10000  5000  5000  3000  5000 10000  5000 10000  5000 10000 \n",
      "  833   834   835   836   837   838   839   840   841   842   843   844   845 \n",
      "10000 10000  5000 10000 10000 10000 10000 30000 10000 10000  3000  3000 10000 \n",
      "  846   847   848   849   850   851   852   853   854   855   856   857   858 \n",
      " 5000  5000  5000  3000  5000 10000 10000  3000  5000  5000  5000 10000  5000 \n",
      "  859   860   861   862   863   864   865   866   867   868   869   870   871 \n",
      "10000  5000  5000 10000  5000 10000  5000  5000 10000  3000 10000 10000 10000 \n",
      "  872   873   874   875   876   877   878   879   880   881   882   883   884 \n",
      "10000 10000 10000  5000  5000  5000 10000  5000  5000 10000  5000  5000  5000 \n",
      "  885   886   887   888   889   890   891   892   893   894   895   896   897 \n",
      " 5000  5000  5000  5000  5000  5000  5000  5000  5000  5000  5000  5000  5000 \n",
      "  898   899   900   901   902   903   904   905   906   907   908   909   910 \n",
      " 5000  5000  5000  5000 10000  5000  5000 10000 10000  5000  5000 10000  5000 \n",
      "  911   912   913   914   915   916   917   918   919   920   921   922   923 \n",
      "10000  5000 10000  3000  5000  5000  3000  5000  5000  3000  5000 10000 10000 \n",
      "  924   925   926   927   928   929   930   931   932   933   934   935   936 \n",
      " 3000  5000 10000 10000  5000 10000  5000 10000 10000 10000  5000 10000 10000 \n",
      "  937   938   939   940   941   942   943   944   945   946   947   948   949 \n",
      "10000  5000  5000  5000  5000 10000  5000 10000 10000  5000  5000 10000  5000 \n",
      "  950   951   952   953   954   955   956   957   958   959   960   961   962 \n",
      "10000  5000 10000  5000 10000  5000 10000 10000  5000  5000 10000  5000 10000 \n",
      "  963   964   965   966   967   968   969   970   971   972   973   974   975 \n",
      " 5000 10000 10000 10000  5000 10000 10000  5000  5000 10000 10000  5000  5000 \n",
      "  976   977   978   979   980   981   982   983   984   985   986   987   988 \n",
      "10000 10000 10000 10000 10000  5000 10000 10000 10000 10000 10000 10000 10000 \n",
      "  989   990   991   992   993   994   995   996   997   998   999  1000 \n",
      " 5000  5000 10000  5000 10000 10000  5000  5000  5000 10000 10000 10000 \n",
      "Levels: 3000 5000 7622.45 10000 14000 20000 30000 60000 130000 160000\n"
     ]
    }
   ],
   "source": [
    "GetModelCapaciteEmprunt(apprentissage, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"generating new model ...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in split.default(x = seq_len(nrow(x)), f = f, drop = drop, ...):\n",
      "\"la taille de données n'est pas un multiple de la variable découpée\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in cbind(yval2, yprob, nodeprob): le nombre de lignes des matrices doit correspondre (voir argument 2)\n",
     "output_type": "error",
     "traceback": [
      "Error in cbind(yval2, yprob, nodeprob): le nombre de lignes des matrices doit correspondre (voir argument 2)\nTraceback:\n",
      "1. GetModelCapaciteSecteurOne(apprentissage, 10)",
      "2. validationCroiseeGetModel(donneesAppr, partitions, \"X1\")   # at line 439 of file <text>",
      "3. rpart(modalData_step2$X1 ~ ., modalData_step2, method = \"class\", \n .     control = rpart.control(minsplit = 5, cp = 0))   # at line 353 of file <text>",
      "4. cbind(yval2, yprob, nodeprob)"
     ]
    }
   ],
   "source": [
    "GetModelCapaciteSecteurOne(apprentissage, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
